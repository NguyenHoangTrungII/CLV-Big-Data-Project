version: '3.8'

services:

  zookeeper:
    image: confluentinc/cp-zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
        - hbase-network

  # zoo:
  #     image: zookeeper:3.4.10
  #     container_name: zoo
  #     # environment:
  #       # ZOO_MY_ID: 1
  #       # ZOO_SERVERS: server.1=0.0.0.0:2888:3888
  #     ports:
  #       - 2186:2181
  # zookeeper1:
  #   image: confluentinc/cp-zookeeper
  #   container_name: zookeeper1
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     # ZOOKEEPER_SERVER_ID: 1
  #     ZOOKEEPER_TICK_TIME: 2000
  #     # ZOOKEEPER_SERVERS: "zookeeper1:2888:3888,zookeeper2:2888:3888" 
  #   ports:
  #     - "2182:2181"

  # zookeeper2:
  #   image: confluentinc/cp-zookeeper
  #   container_name: zookeeper2
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #     ZOOKEEPER_SERVER_ID: 2
  #     # ZOOKEEPER_SERVERS: "zookeeper1:2888:3888,zookeeper2:2888:3888" 
  #   ports:
  #     - "2183:2181"  # Khác cổng để tránh xung đột

  zookeeper1:
    image: zookeeper:3.4.10   
    container_name: zookeeper1
    ports:
      - "2182:2181"  
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: "server.1=zookeeper1:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper1/data
      - /tmp/zookeeper1/datalog
    networks:
      - hbase-network

  zookeeper2:
    image: zookeeper:3.4.10
    container_name: zookeeper2
    ports:
      - "2183:2181"  
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: "server.2=zookeeper2:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper2/data
      - /tmp/zookeeper2/datalog
    networks:
      - hbase-network

  zookeeper3:
    image: zookeeper:3.4.10
    container_name: zookeeper3
    ports:
      - "2184:2181"  
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: "server.3=zookeeper3:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper3/data
      - /tmp/zookeeper3/datalog
    networks:
      - hbase-network

  hbase-master:
    build:
      context: ./
      # dockerfile: stream_process/hbase_distribute/hbase_master/Dockerfile
      dockerfile: stream_process/hbase/Dockerfile
    container_name: hbase-master
    hostname: hbase-master
    ports:
      - "9090:9090"
      - "16010:16010"  # UI port for HBase Master
      - "16000:16000"  # Master port for HBase communication
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_MASTER=hbase-master:16000
      - HBASE_REGIONSERVER=hbase-regionserver:16020
      - HBASE_MASTER_OPTS=-Dhbase.master.info.port=16010
      - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2,zookeeper3
    depends_on:
      - namenode
      # - zoo
      - zookeeper1
      - zookeeper2
      - zookeeper3
    env_file:
      - ./stream_process/hbase_distribute/hbase-distributed-local.env
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:16010"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    command: /bin/sh -c "/usr/local/hbase/bin/hbase master start && tail -f /dev/null"
    networks:
      - hbase-network

  hbase-regionserver:
    build:
      context: ./
      # dockerfile: stream_process/hbase_distribute/hbase_regionserver/Dockerfile
      dockerfile: stream_process/hbase/Dockerfile
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    ports:
      - "16020:16020"  # RegionServer port
      - "16030:16030"
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_MASTER=hbase-master:16000
      - HBASE_REGIONSERVER_OPTS=-Dhbase.regionserver.info.port=16030
      - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2, zookeeper3
      - HBASE_REGIONSERVER_HOSTNAME=hbase-regionserver.clv-big-data-project_hbase-network
    depends_on:
      - hbase-master
    env_file:
      - ./stream_process/hbase_distribute/hbase-distributed-local.env
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:16030"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    command: /bin/sh -c "/usr/local/hbase/bin/hbase regionserver start && tail -f /dev/null"
    networks:
      - hbase-network

  # hbase:
  #   # image: bde2020/hbase-standalone:1.0.0-hbase1.2.6
  #   build:
  #     context: ./stream_process/hbase
  #   container_name: hbase
  #   volumes:
  #     - hbase_data:/hbase-data
  #     - hbase_zookeeper_data:/zookeeper-data
  #   ports:
  #     - 16000:16000
  #     - 16010:16010
  #     - 16020:16020
  #     - 16030:16030
  #     - 2888:2888
  #     - 3888:3888
  #     - 2188:2181
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
  #   env_file:
  #     - ./stream_process/hbase/hbase-standalone.env
  #   command: /bin/sh -c "/usr/local/hbase/bin/start-hbase.sh && tail -f /dev/null"

  kafka:
    build:
      context: ./stream_process/kafka
    container_name: kafka
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:2181
      # KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://172.27.254.108:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093
    ports:
      - "9093:9093"
    depends_on:
      - zookeeper1
    # networks:
    #   - kafka-network
    networks:
      - hbase-network

  # hbase:
  #   build:
  #     context: ./stream_process/hbase
  #   container_name: hbase
  #   # ports:
  #   #   - "16010:16010"
  #   #   - "2182:2181"
  #   #   - "9090:9090"
  #   # depends_on:
  #   #   - zookeeper
  #   ports:
  #     - "16010:16010"   # UI port for HBase Master
  #     - "16000:16000"   # Master port for HBase communication
  #     - "16020:16020"   # RegionServer port
  #     - "2182:2181"     # Zookeeper port (if applicable)
  #     - "9090:9090"     # Custom port for monitoring or others
  #   depends_on:
  #     - zookeeper
  #   environment:
  #     - HBASE_MASTER=hbase:16000
  #     - HBASE_REGIONSERVER=hbase:16020
  #   command: /bin/sh -c "/usr/local/hbase/bin/start-hbase.sh && tail -f /dev/null"
  
  # hbase-master:
  #   build:
  #     context: ./stream_process/hbase
  #   container_name: hbase-master
  #   ports:
  #     - "16010:16010"    # UI port for HBase Master
  #     - "16000:16000"    # Master port for HBase communication
  #   environment:
  #     - HBASE_MASTER=hbase-master:16000
  #     - HBASE_REGIONSERVER=hbase-regionserver:16020
  #     - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2
  #   depends_on:
  #     - zookeeper1
  #     - zookeeper2
  #   command: /bin/sh -c "/usr/local/hbase/bin/start-hbase.sh master && tail -f /dev/null"
  
  # hbase-regionserver:
  #   build:
  #     context: ./stream_process/hbase
  #   container_name: hbase-regionserver
  #   ports:
  #     - "16020:16020"    # RegionServer port
  #   environment:
  #     - HBASE_MASTER=hbase-master:16000
  #     - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2
  #   depends_on:
  #     - hbase-master
  #   command: /bin/sh -c "/usr/local/hbase/bin/start-hbase.sh regionserver && tail -f /dev/null"
  
  # hadoop:
  #   build:
  #     # context: ./batch_process/hadoop
  #     context: ./ # Đảm bảo rằng context trỏ đến thư mục gốc của dự án
  #     dockerfile: batch_process/hadoop/Dockerfile
  #   container_name: hadoop
  #   ports:
  #     - "50070:50070"
  #     - "8088:8088"

  namenode:
    build:
      context: ./
      dockerfile: batch_process/hadoop/Dockerfile
    container_name: namenode
    ports:
      - "50070:50070"   # Web UI của NameNode
      - "9000:9000"     # Hadoop HDFS
    environment:
      - HADOOP_HOME=/usr/local/hadoop
      - HADOOP_NAMENODE_HOST=namenode
      - HADOOP_NAMENODE_PORT=9000
    volumes:
      - hadoop_namenode_data:/usr/local/hadoop/data/name
    command: ["bash", "-c", "service ssh start && /usr/local/hadoop/sbin/start-dfs.sh && /usr/local/hadoop/sbin/start-yarn.sh && tail -f /dev/null"]
    networks:
      - hbase-network
    
  datanode:
    build:
      context: ./
      dockerfile: batch_process/hadoop/Dockerfile
    container_name: datanode
    depends_on:
      - namenode
    environment:
      - HADOOP_HOME=/usr/local/hadoop
      - HADOOP_NAMENODE_HOST=namenode
      - HADOOP_NAMENODE_PORT=9000
    volumes:
      - hadoop_datanode_data:/usr/local/hadoop/data/datanode
    command: ["bash", "-c", "service ssh start && /usr/local/hadoop/sbin/hadoop-daemon.sh start datanode && tail -f /dev/null"]
    networks:
      - hbase-network

  spark-master:
    # image: clv-big-data-project-spark:latest
    build:
     context: ./batch_process/spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER=spark://spark-master:7077
      # - SPARK_MASTER=spark://172.20.40.142:7077
      # - SPARK_LOCAL_HOSTNAME=172.20.40.142  # Địa chỉ IP của máy host
    ports:
      - "7077:7077"   # Spark master port
      - "8080:8080"   # Spark UI port
    # networks:
    #   - spark-network
    command: ["/usr/local/spark/sbin/start-master.sh", "--host", "0.0.0.0", "--port", "7077", "--webui-port", "8080"]
    networks:
      - hbase-network

  spark-worker:
    # image: clv-big-data-project-spark:latest
    build:
      context: ./batch_process/spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4g      # Tăng bộ nhớ (ví dụ, 4GB)
      - SPARK_WORKER_CORES=4
      # - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master
    # networks:
    #   - spark-network
    ports:
      - "8081:8081"   # Spark worker UI port
    command: ["/bin/sh", "-c", "until nc -z spark-master 7077; do echo waiting for spark-master; sleep 2; done; /usr/local/spark/sbin/start-worker.sh spark://spark-master:7077"]
    networks:
      - hbase-network
      
  postgres:
    build:
      context: ./batch_process/postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  airflow:
    build:
      context: ./batch_process/airflow
    container_name: airflow
    ports:
      - "8082:8080"
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    depends_on:
      - postgres
    networks:
      - hbase-network

  # python-runner:
  #   build:
  #     context: ./python_runner
  #   image: 5cd2f0903f9e  
  #   container_name: python-runner-container-newest-1
  #   volumes:
  #     - ./:/app
  #   tty: true
  #   # network_mode: "host"
  #   networks:
  #     - default

networks:
  hbase-network:
    driver: bridge
  default:
    driver: bridge
volumes:
  hadoop_namenode_data:
  hadoop_datanode_data:
  hbase_data:
  hbase_zookeeper_data: