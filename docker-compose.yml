version: '3.8'

services:

  zookeeper:
    image: confluentinc/cp-zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
        - hbase-network

  zookeeper1:
    image: zookeeper:3.4.10   
    container_name: zookeeper1
    ports:
      - "2182:2181"  
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: "server.1=zookeeper1:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper1/data
      - /tmp/zookeeper1/datalog
    networks:
      - hbase-network

  zookeeper2:
    image: zookeeper:3.4.10
    container_name: zookeeper2
    ports:
      - "2183:2181"  
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: "server.2=zookeeper2:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper2/data
      - /tmp/zookeeper2/datalog
    networks:
      - hbase-network

  zookeeper3:
    image: zookeeper:3.4.10
    container_name: zookeeper3
    ports:
      - "2184:2181"  
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: "server.3=zookeeper3:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - /tmp/zookeeper3/data
      - /tmp/zookeeper3/datalog
    networks:
      - hbase-network

  hbase-master:
    build:
      context: ./
      dockerfile: stream_process/hbase/Dockerfile
    container_name: hbase-master
    hostname: hbase-master
    ports:
      - "9090:9090"
      - "16010:16010"  # UI port for HBase Master
      - "16000:16000"  # Master port for HBase communication
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_MASTER=hbase-master:16000
      - HBASE_REGIONSERVER=hbase-regionserver:16020
      - HBASE_MASTER_OPTS=-Dhbase.master.info.port=16010
      - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2,zookeeper3
    depends_on:
      - namenode
      # - zoo
      - zookeeper1
      - zookeeper2
      - zookeeper3
    env_file:
      - ./stream_process/hbase_distribute/hbase-distributed-local.env
    command: /bin/sh -c "/usr/local/hbase/bin/hbase master start && /usr/local/hbase/bin/hbase-daemon.sh start thrift && tail -f /dev/null"
    networks:
      - hbase-network

  hbase-regionserver:
    build:
      context: ./
      # dockerfile: stream_process/hbase_distribute/hbase_regionserver/Dockerfile
      dockerfile: stream_process/hbase/Dockerfile
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    ports:
      - "16020:16020"  # RegionServer port
      - "16030:16030"
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_MASTER=hbase-master:16000
      - HBASE_REGIONSERVER_OPTS=-Dhbase.regionserver.info.port=16030
      - HBASE_ZOOKEEPER_QUORUM=zookeeper1,zookeeper2, zookeeper3
      - HBASE_REGIONSERVER_HOSTNAME=hbase-regionserver.clv-big-data-project_hbase-network
    depends_on:
      - hbase-master
    env_file:
      - ./stream_process/hbase_distribute/hbase-distributed-local.env
    command: /bin/sh -c "/usr/local/hbase/bin/hbase regionserver start && /usr/local/hbase/bin/hbase-daemon.sh start thrift && tail -f /dev/null"
    networks:
      - hbase-network

  kafka:
    build:
      context: ./stream_process/kafka
    container_name: kafka
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:2181
      # KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://172.27.179.20:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093
    ports:
      - "9093:9093"
    volumes:
      - ./stream_process/kafka/kafka_config/server.properties:/usr/local/kafka/config/server.properties
    depends_on:
      - zookeeper1
    networks:
      - hbase-network

  namenode:
    # image: gelog/hadoop
    build:
      context: ./batch_process/hadoop
      dockerfile: Dockerfile
    container_name: namenode
    ports: 
      - "50070:50070"
    command: hdfs namenode
    hostname: hdfs-namenode
    networks:
      - hbase-network

  datanode:
    # image: gelog/hadoop
    build:
      context: ./batch_process/hadoop
      dockerfile: Dockerfile
    container_name: datanode
    command: hdfs datanode
    ports:
  # The host port is randomly assigned by Docker, to allow scaling to multiple DataNodes on the same host
       - "50075:50075"
    links:
      - namenode:hdfs-namenode
    networks:
      - hbase-network

  secondarynamenode:
    # image: gelog/hadoop
    build:
      context: ./batch_process/hadoop
      dockerfile: Dockerfile
    container_name: secondarynamenode
    command: hdfs secondarynamenode
    ports:
      - "50090:50090"
    links:
      - namenode:hdfs-namenode
    networks:
      - hbase-network
      
  spark-master:
    # image: clv-big-data-project-spark:latest
    build:
     context: ./batch_process/spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER=spark://172.27.179.20:7077
      # - SPARK_MASTER=spark://172.20.40.142:7077
      # - SPARK_LOCAL_HOSTNAME=172.20.40.142  # Địa chỉ IP của máy host
    ports:
      - "7077:7077"   # Spark master port
      - "8080:8080"   # Spark UI port
    # networks:
    #   - spark-network
    command: ["/usr/local/spark/sbin/start-master.sh", "--host", "0.0.0.0", "--port", "7077", "--webui-port", "8080"]
    networks:
      - hbase-network

  spark-worker:
    # image: clv-big-data-project-spark:latest
    build:
      context: ./batch_process/spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://172.27.179.20:7077
      - SPARK_WORKER_MEMORY=4g      # Tăng bộ nhớ (ví dụ, 4GB)
      - SPARK_WORKER_CORES=4
      # - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master
    # networks:
    #   - spark-network
    ports:
      - "8081:8081"   # Spark worker UI port
    command: ["/bin/sh", "-c", "until nc -z spark-master 7077; do echo waiting for spark-master; sleep 2; done; /usr/local/spark/sbin/start-worker.sh spark://172.27.179.20:7077"]
    networks:
      - hbase-network
      
  postgres:
    build:
      context: ./batch_process/postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - hbase-network

  airflow:
    build:
      context: ./batch_process/airflow
    container_name: airflow
    ports:
      - "8082:8080"
    environment:
      - AIRFLOW_HOME=/usr/local/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=./orchestration/dags
      - PYTHONPATH=/usr/local/airflow/batch_process
    volumes:
      - airflow_data:/usr/local/airflow
      - ./orchestration:/usr/local/airflow/orchestration
      - ./batch_process:/usr/local/airflow/batch_process
    depends_on:
      - postgres
    networks:
      - hbase-network
    command: ["airflow", "webserver", "--port", "8080"]

  # python-runner:
  #   build:
  #     context: ./python_runner
  #   image: 5cd2f0903f9e  
  #   container_name: python-runner-container-newest-1
  #   volumes:
  #     - ./:/app
  #   tty: true
  #   # network_mode: "host"
  #   networks:
  #     - default

networks:
  hbase-network:
    driver: bridge
  default:
    driver: bridge
volumes:
  hadoop_namenode_data:
  hadoop_datanode_data:
  hbase_data:
  hbase_zookeeper_data:
  airflow_data: